
# pandas


- [pandas](#pandas)
- [数据读取](#%e6%95%b0%e6%8d%ae%e8%af%bb%e5%8f%96)
- [行处理和列处理](#%e8%a1%8c%e5%a4%84%e7%90%86%e5%92%8c%e5%88%97%e5%a4%84%e7%90%86)
- [条件](#%e6%9d%a1%e4%bb%b6)
- [排序](#%e6%8e%92%e5%ba%8f)
- [分组汇总](#%e5%88%86%e7%bb%84%e6%b1%87%e6%80%bb)
- [多个dataframe](#%e5%a4%9a%e4%b8%aadataframe)
- [shift](#shift)
  - [数据变换](#%e6%95%b0%e6%8d%ae%e5%8f%98%e6%8d%a2)
- [拆分](#%e6%8b%86%e5%88%86)
- [保存和格式化](#%e4%bf%9d%e5%ad%98%e5%92%8c%e6%a0%bc%e5%bc%8f%e5%8c%96)
- [dask](#dask)
- [pandas to mysql](#pandas-to-mysql)
  

# 数据读取

```python
file_name = "../track2/final_track2_train.txt"
column_names = ["uid", "user_city",
    "item_id", "author_id", "item_city", "channel",
    "finish", "like", "music_id", "device", "time",
    "duration_time"]
df = pd.read_csv(file_name, sep='\t', header=None, names=column_names)
```
列重命名
```python
name_map = {"col1_old": "col1_new", "col2_old": "col2_new"}
df.rename(columns=name_map)
```

列类型
```python
float_cols = ['col1', 'col2']
df[float_cols] = df[float_cols].apply(lambda x:x.astype(float))
```

# 行处理和列处理


> apply函数

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html

- axis=0 （默认） ，  func的输入为DataFrame的一列， 类型为Series， index为DataFrame的行索引
- axis=1时， func的输入为DataFrame的一行， 类型为Series， index为DataFrame的列索引
```python
df = pd.DataFrame([[4, 9],] * 3, columns=['A', 'B'])
df.apply(lambda x: x.name) # 获取列名
```

> applymap函数
> 
applymap works element-wise on a DataFrame
```
df[['col1','col2']] = df[['col1','col2']].applymap("{0:.2f}%".format)
```


```python
# 统计每列有多少种不同的取值
def uniq_count(series):
    return len(np.unique(series))
df.apply(uniq_count, axis=0)
```

```python
# 统计每列的最大最小值
def minMax(x):
    return pd.Series(index=['min','max'], data=[x.min(),x.max()])
df.apply(minMax)
```

```python
# 多列应用同一个函数, cols 为list
df[cols] = df[cols].apply(lambda x: x.clip(lower=0))
```

# 条件

in of a Series checks whether the value is in the index
```python
s = pd.Series(list('abc'))
'a' in s # False, 
'a' in s.values
```

isin
```python
df['col1'].isin(['a1', 'a2'])
```



# 排序
```
df = df.sort_values(by=['col1', 'col2'], ascending=True)
```

# 分组汇总
使用groupby时注意groupby的列类型是否为category， 如果有category类型，会使得结果中包含完全组合的分组结果产生冗余
```python
# 每个组有多少条数据
df.groupby(["uid"]).size().reset_index(name='counts')
df.groupby(["uid"], as_index=False, observed=True).agg({"charge":np.sum})
```

使用agg聚合函数
```python
df.groupby('group').agg({'a':['sum', 'max'], 
                         'b':'mean', 
                         'c':np.average, 
                         'd':lambda x: x.max() - x.min()})
```


DataFrame.apply 和 GroupBy.apply 不同， 
- DataFrame.apply 函数传递给自定义函数的是Series对象， apply有自己的参数 
- GroupBy.apply 函数传递给自定义函数的是DataFrame对象， apply会将参数也传递给自定义函数
```python
def item_count(df):
    return len(np.unique(df["item_id"]))
user_item_count = df[["uid","item_id"]].groupby(["uid"]).apply(item_count)
```

```python
# use apply and return Series, Use the Series index as labels for the new columns

eg1: 
def f(x):
    d = {}
    d['a_sum'] = x['a'].sum()
    d['a_max'] = x['a'].max()
    d['b_mean'] = x['b'].mean()
    d['c_d_prodsum'] = (x['c'] * x['d']).sum()
    return pd.Series(d, index=['a_sum', 'a_max', 'b_mean', 'c_d_prodsum'])

df.groupby('group').apply(f)


          a_sum     a_max    b_mean  c_d_prodsum
group                                           
0      0.560541  0.507058  0.418546     0.118106
1      0.187757  0.157958  0.887315     0.276808


eg2:
like_info = data[['like']].groupby(['like']).size()

def extract_feature(df):
    d = {}
    d['user_city_pv'] = len(df['like'])
    d['user_city_clk'] = sum(df['like'])
    d['user_city_pvbeta'] = d['user_city_pv'] + like_info[0] + like_info[1]
    d['user_city_clkbeta'] = d['user_city_pv'] + like_info[1]
    d['user_city_ctrbeta'] = d['user_city_clkbeta'] / d['user_city_pvbeta']
    return pd.Series(d, index=['user_city_pv', 'user_city_clk', 
                               'user_city_pvbeta', 'user_city_clkbeta',
                               'user_city_ctrbeta'])

data[['user_city', 'like']].groupby(['user_city']).apply(extract_feature)   
```

# 多个dataframe
```python
data = data.merge(item_city_data, on=['item_city'], how='left')

# 按行拼接
data = pd.concat([data_train, data_test], axis=0)
```

多个dataframe，按element-wise方式合并成一个dataframe
```python
df1 = df1.applymap(lambda x:[x])
df2 = df2.applymap(lambda x:[x])
colnames = ["x1", "x2"]
(df1 + df2).applymap(lambda x: dict(zip(colnames, x)))
```
使用自定义类，定义__add__操作
```python
class Element(object):
    def __init__(self, name, value):
        self.d = {}
        self.d[name] = value
        
    def __str__(self):
        return str(self.d)
    
    def __add__(self, other):
        return {**self.d, **other.d}
```



# shift


## 数据变换

> 时间数据
```
df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
```
- 10位时间戳，unit使用s
- 13位时间戳，unit使用ms

```
df['time'] = pd.to_datetime(df['time'], format='%Y-%m-%d %H:%M:%S,%f')
```
- 特定格式的字符串转换

> 时区变换

datetime类型的列, 变换为相应时区
```
df['timestamp'].dt.tz_localize('UTC').dt.tz_convert('Asia/Shanghai')
```

# 拆分
按照某列进行分组，拆分成多个dataframe
```
d = dict(tuple(df.groupby('col_group')))

```


# 保存和格式化
格式化
```
df[['col1', 'col2']] = df[['col1', 'col2']].applymap("{:.2f}".format)
```
保存
```
df[['col1', 'col2']].to_csv("data.csv", index=False)
df[['col1', 'col2']].to_csv("data.csv", index=False, float_format='%.2f')
```

# dask

dask 读取hdfs数据
```
import os
import numpy as np
import pandas as pd
import dask.dataframe as dd
pd.set_option('display.max_colwidth', -1)

def read_df():
    # ip and port 可以在etc/hadoop/hdfs-site.xml中找到
    ddf = dd.read_csv('hdfs://ip:port/user/test/2019-09-*/data/part-*', 
                     sep=",",
                     dtype={'predict': 'float64'},
                     include_path_column=True
                     )
    df = ddf.compute()
    df['date'] = df['path'].map(lambda x:os.path.basename(os.path.dirname(os.path.dirname(x))))
    print("len:", len(df))
    return df
```
- include_path_column 区分文件名
- usecols 不同文件数据列数不同，选择共同的子列

# pandas to mysql
```python
import sqlalchemy

class DFToMysql:
    def __init__(self, engine_str):
        self.engine = sqlalchemy.create_engine(engine_str, echo=False, encoding='utf-8')

    def save_data(self, df, insert_columns, update_columns):
        query = """
        INSERT INTO ocpc_cvr ({insert_str})
        VALUES({value_str})
        ON DUPLICATE KEY UPDATE {update_str}
        """.replace('\n', ' ').format(
            insert_str=','.join(list(map(lambda x: "`{}`".format(x), insert_columns))),
            value_str=','.join(["%s"]*len(insert_columns)),
            update_str=','.join(map(lambda x:"{}=%s".format(x), list(map(lambda x: "`{}`".format(x), update_columns))))
        )

        def gen_data():
            for i in range(len(df)):
                insert_value = list(map(lambda x: str(df[x][i]), insert_columns))
                update_value = list(map(lambda x: str(df[x][i]), update_columns))
                result = insert_value + update_value
                yield tuple(result)

        for query_data in gen_data():
            self.engine.execute(query, query_data)

engine_str = 'mysql+pymysql://algo:algo@algo-spark1.jd.163.org/algo'
handle = DFToMysql(engine_str)

df = pd.read_csv("account_scheduling.csv")
insert_columns = "dt,account_id,scheduling_id,show,charge,click,install,pcvr,pcoc,install_fix".split(',')
update_columns = "show,charge,click,install,pcvr,pcoc,install_fix".split(',')
handle.save_data(df, insert_columns, update_columns)
```