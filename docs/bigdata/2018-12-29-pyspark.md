
# spark

- [spark](#spark)
- [spark conf](#spark-conf)
- [pyspark](#pyspark)
  - [use pyspark with spark-submit](#use-pyspark-with-spark-submit)
  - [use pyspark with jupyter notebook](#use-pyspark-with-jupyter-notebook)
  - [pyspark python version on yarn](#pyspark-python-version-on-yarn)
- [spark-sql](#spark-sql)
- [读取](#读取)
  - [读取csv文件](#读取csv文件)
  - [读取任意文件](#读取任意文件)
- [schema](#schema)
- [map and flatMap](#map-and-flatmap)
- [function](#function)
- [选择列](#选择列)
- [修改列](#修改列)
- [过滤行](#过滤行)
- [去重](#去重)
- [排序](#排序)
- [RDD聚合](#rdd聚合)
- [分组聚合](#分组聚合)
- [join](#join)
- [RDD 和 DataFrame 转换](#rdd-和-dataframe-转换)
- [保存rdd到文件](#保存rdd到文件)
- [pyspark python](#pyspark-python)
- [spark streaming](#spark-streaming)
  - [Discretized Stream (DStream)](#discretized-stream-dstream)
- [Structured Streaming](#structured-streaming)
  - [数据源](#数据源)
  - [join](#join-1)
  - [输出](#输出)
- [spark-shell scala](#spark-shell-scala)
- [spark-shell](#spark-shell)
- [spark job stage](#spark-job-stage)


# spark conf
参数的生效顺序， 优先级由高到低
1. SparkConf 程序中使用SparkConf对象进行配置
2. spark-submit 的命令行参数
3. conf/spark-defaults.conf

部署相关的参数，例如spark.driver.memory使用SparkConf对象进行配置可能不起作用

conf/spark-env.sh
```
export JAVA_HOME=

export HADOOP_HOME=
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

export SPARK_HOME=
export SPARK_CLASSPATH="${SPARK_HOME}/lib/mysql-connector-java-8.0.17.jar:${SPARK_HOME}/lib/hadoop-lzo-0.4.13.jar:${SPARK_HOME}/lib/hive-serde-3.1.0.jar:${SPARK_HOME}/lib/json-udf-1.3.7-jar-with-dependencies.jar:${SPARK_HOME}/lib/json-serde-1.3.7-jar-with-dependencies.jar:$SPARK_CLASSPATH"
```
spark未自带但常用的jar包
```
cd $SPARK_HOME/jars
wget https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.17/mysql-connector-java-8.0.17.jar
wget http://knowitall.cs.washington.edu/maven2/org/apache/hadoop/hadoop-lzo/0.4.13/hadoop-lzo-0.4.13.jar
```
spark sql 使用hive

将hive/conf/hive-site.xml 拷贝到spark/conf/目录下

# pyspark

## use pyspark with spark-submit

如何获得SparkSession

```python
from pyspark import SparkConf
from pyspark.sql import SparkSession, SQLContext
from pyspark.sql.types import *
import pyspark.sql.functions as f

conf = SparkConf()
spark = SparkSession\
    .builder\
    .appName("AppName")\
    .config(conf=conf) \
    .enableHiveSupport() \
    .getOrCreate()
sc = spark.sparkContext
sqlconext = SQLContext(sc)
```


- 在object中不要将Rdd直接声明为变量，可以使用函数, 避免序列化造成的问题
```scala
import org.apache.spark.{ SparkConf, SparkContext }
import org.apache.spark.sql.SparkSession

object Spark {
    val conf = new SparkConf()
    val spark = SparkSession.builder()
        .config(conf)
        .enableHiveSupport()
        .getOrCreate()
}
```

## use pyspark with jupyter notebook

```
import os
# os.environ['SPARK_HOME']
# os.environ['JAVA_HOME']
os.environ['PYSPARK_SUBMIT_ARGS'] = "--master local[2] pyspark-shell"

from pyspark.sql import SparkSession, SQLContext
from pyspark.sql.types import *
import pyspark.sql.functions as f

spark = SparkSession \
    .builder \
    .enableHiveSupport() \
    .getOrCreate()
sc = spark.sparkContext
sqlconext = SQLContext(sc)
```

## pyspark python version on yarn
- 打包python https://conda.github.io/conda-pack/spark.html

打包python
```
conda create -y -n example python=3.5 numpy pandas scikit-learn
conda activate example
conda pack -o environment.tar.gz
```

client模式启动
```
$ PYSPARK_DRIVER_PYTHON=`which python` \
PYSPARK_PYTHON=./environment/bin/python \
spark-submit \
--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./environment/bin/python \
--conf spark.executorEnv.PYSPARK_PYTHON=./environment/bin/python \
--master yarn \
--deploy-mode client \
--archives environment.tar.gz#environment \
script.py
```

cluster模式启动
```
$ PYSPARK_PYTHON=./environment/bin/python \
spark-submit \
--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./environment/bin/python \
--conf spark.executorEnv.PYSPARK_PYTHON=./environment/bin/python \
--master yarn \
--deploy-mode cluster \
--archives environment.tar.gz#environment \
script.py
```

classpath 调试
```
--conf 'spark.driver.extraJavaOptions=-verbose:class' \
--conf 'spark.executor.extraJavaOptions=-verbose:class' \
```


```
import os
os.environ['SPARK_HOME'] = "/home/appops/spark-2.4.3-bin-hadoop2.7"
os.environ['PYSPARK_PYTHON'] = "./python373/bin/python"
os.environ['PYSPARK_SUBMIT_ARGS'] = "--master yarn --archives ../python373.zip#python373 pyspark-shell"

conf = SparkConf()
spark = SparkSession \
    .builder \
    .config(conf=conf) \
    .enableHiveSupport() \
    .getOrCreate()
sc = spark.sparkContext
sqlconext = SQLContext(sc)

print(sc.applicationId)
```

# spark-sql
确保hive命令行能正常工作， 复制hive/conf/hive-site.xml 到spark/conf/目录下，通过spark-sql命令行确保spark访问hive能够正常工作(例如执行show databases;)

```
from pyspark.sql import SparkSession
spark = SparkSession \
    .builder \
    .appName("Python Spark SQL Hive integration example") \
    .enableHiveSupport() \
    .getOrCreate()

# show databases
spark.catalog.listDatabases()
spark.catalog.listTables(dbName="test")
# or
spark.sql("show databases").show()
spark.sql("use test").show()
spark.sql("show tables").show()

```

# 读取

## 读取csv文件

python读取
```python
#file_name = "hdfs:///user/
#file_name = "file:///home/

# method 1:
# input_schema = StructType()
# for col in column_names:
#     input_schema.add(StructField(col, StringType(), True))              
# method 2:
# input_schema = spark.read.schema("col0 INT, col2 DOUBLE")

df = spark.read.csv(file_name, schema=input_schema, sep='\t')
```

scala读取，csv带有header, 返回DataFrame
```scala
 val df = spark.read
         .format("csv")
         .option("header", "true") //first line in file has headers
         .option("mode", "DROPMALFORMED")
         .load("hdfs:///csv/file/dir/file.csv")
```

## 读取任意文件
将文件读取为rdd，然后转换为dataframe， 使用rdd的map将文件行解析为结构化的字段

```python
def read_log_df():
    log_rdd = sc.textFile(log_dir)
    log_rdd = log_rdd.map(lambda x: map_log_data(x))
    log_rdd = log_rdd.filter(lambda x: x != "-")
    log_df = spark.createDataFrame(log_rdd, schema=["col1", "col2"])
    return log_df
log_df = read_log_df()
```

```scala
sc.textFile() # return org.apache.spark.rdd.RDD[String]
spark.read.textFile() # org.apache.spark.sql.Dataset[String]
```

# schema
```scala
spark.table(eventDataTableName) // org.apache.spark.sql.DataFrame
    .schema // org.apache.spark.sql.types.StructType
    .fields // Array[org.apache.spark.sql.types.StructField]
    .map(_.name) // Array[String]

// Array[String]
spark.table(eventDataTableName).schema.fields.map(_.name)
```

# map and flatMap
flatMap 会自动过滤空列表
```
rdd = sc.parallelize([2, 3, 4, None])
def tmp_func(x):
    if x is None:
        return []
    else:
        return range(1, x)
rdd.flatMap(tmp_func).collect()
# output: [1, 1, 2, 1, 2, 3]
```

# function
- array
- explode

# 选择列
```python
df = df[['clk', 'site', 'category', 'location', 'ctr']]
```

selectExpr相比select而言，可以解析sql表达式
```scala
df.selectExpr("colA", "colB as newName", "abs(colC)")
```

# 修改列
```python
df = df.withColumn('ctr', f.col('ctr')/1000000)

# 增加常量列
df = df.withColumn('constant', f.lit(10))

# 条件修改, 使用udf
from pyspark.sql.types import StringType
from pyspark.sql.functions import udf

def modify_values(r):
    if r == "A" or r =="B":
        return "dispatch"
    else:
        return "non-dispatch"
ol_val = udf(modify_values, StringType())
new_df = df.withColumn("wo_flag",ol_val(df.wo_flag))

# 条件修改，使用when, otherwise
data_df = data_df.withColumn("click_label", f.when(f.col("click_label").isNull(), 0).otherwise(1))
```

string to array
```scala
df.withColumn("b", split(col("b"), ",").cast("array<long>"))

val tolong = udf((value : String) => value.split(",").map(_.toLong))
df.withColumn("newB", tolong(col("b"))).show
```

# 过滤行
```python
df = df.filter(
        (f.col('site') == '1') & 
        (f.col('category') == 'FOCUS2')
        )
```

集合过滤isin
```scala
val col1_set = spark.table(LogTableName)
    .filter(col("tag") === "1")
    .select("col1")
    .distinct()
    .collect()
    .map(_.getString(0))

spark.table(LogTableName)
    .filter($"col1".isin(col1_set: _*))
```

# 去重
```
df = df.dropDuplicates(['name', 'height'])
```

```
df.groupby(['col1', 'col2']).agg(
    first(col('col3')).alias('col3'),
    first(col('col4')).alias('col4')
)
```

```scala
df.withColumn("rank", row_number().over(
        Window.partitionBy(
            col("col1"), col("col2"), col("col3"))
        .orderBy(col("col4").asc, col("col5").asc)))
    .filter(col("rank") === 1)
```

# 排序
```
df = df.orderBy(["age", "name"], ascending=[0, 1])
http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy
```

# RDD聚合
- reduceByKey 需要一个聚合函数， 先本地mapper处聚合，再在reducer处进行聚合
- groupByKey 会将RDD的value聚合成一个sequence, 想要计算聚合值需要继续使用map处理
- combineByKey 聚合之后的数据类型可以与输入值的类型不同

# 分组聚合
```python
df = df.groupBy(['location']).agg(
        f.count('clk').alias("click"),
        f.sum('clk'), 
        f.sum('ctr'), 
        f.sum('ctr')/f.sum('clk'))
```
- 使用pyspark.sql.functions里的聚合函数

- pandas_udf 分组数据会转化为pd.DataFrame， 需要注意内存是否放的下分组数据， pandas_udf设置的返回类型
需要与函数返回的pd.DataFrame类型一致
```python
import pandas as pd
from pyspark.sql.types import *
from pyspark.sql.functions import pandas_udf, PandasUDFType

uid_schema = StructType()
uid_schema.add(StructField("uid", LongType(), True))
uid_schema.add(StructField("uid_finish_pv", LongType(), True))
uid_schema.add(StructField("uid_finish_clk", LongType(), True))
print("uid schema:", uid_schema)
@f.pandas_udf(uid_schema, f.PandasUDFType.GROUPED_MAP)
def uid_extract(pdf):
    d = {}
    d['uid'] = [pdf['uid'][0]]
    d['uid_finish_pv'] =  [len(pdf['finish'])]
    d['uid_finish_clk'] = [sum(pdf['finish'])]
    df = pd.DataFrame(d, columns=uid_schema.fieldNames())
    return df
uid_df = data_df.groupby(['uid']).apply(uid_extract)
```

https://chilunhuang.github.io/posts/53705/
- collect_list, collect_set
- flatten, array_distinct

# join
```python
data_df = show_df.join(click_df, on=["request_id", "location"], how='left')

# 当join的两张表存在相同列名时，join后不容易选择列，使用表名的方式进行选择
show_df = show_df.alias("imp")
click_df = click_df.withColumn("click_label", f.lit(1))
click_df = click_df.alias("click")
# 选择曝光表的所有列，选择点击表的标签列
data_df = show_df.join(click_df, on=["id"], how='left').select("imp.*", "click.click_label")
# 修改标签列里的None值
data_df = data_df.withColumn("click_label", f.when(f.col("click_label").isNull(), 0).otherwise(1))
```

# RDD 和 DataFrame 转换
```
from pyspark.sql import SparkSession, SQLContext
from pyspark.sql.types import *

spark = SparkSession\
    .builder\
    .appName("StatLocationPctr")\
    .getOrCreate()
sc = spark.sparkContext
sqlContext = SQLContext(sc)

# DataFrame to RDD
df.rdd 

# RDD to DataFrame
input_schema = StructType()
input_schema.add(StructField("feature", StringType(), True))
input_schema.add(StructField("value", StringType(), True))
df = sqlContext.createDataFrame(rdd, input_schema)

df = sc.parallelize([
    (1, "foo", 2.0, "2016-02-16"),
    (2, "bar", 3.0, "2016-02-16")
]).toDF(["id", "x", "y", "date"])
```


# 保存rdd到文件
```
df.coalesce(1).write.save(output_dir, format="csv", sep='\t')
df.repartition(1).write.save(output_dir, format="csv", sep='\t')
df.rdd.saveAsTextFile(output_dir)
rdd.saveAsTextFile(output_dir)
rdd.repartition(1).saveAsTextFile(output_dir) 

# 按分区存储文件
df.write.partitionBy('year', 'month').save(dict_output_dir, format="csv", sep='\t')
```
需要特定格式的输出时， 可以使用map方法先拼接成特定格式的字符串，然后再输出

在保存df之前，如果对df进行了排序， repartition会打乱顺序， coalesce不会


# pyspark python


export SPARK_HOME=~/spark-2.4.0-bin-hadoop2.7
或者在~/spark-2.4.0-bin-hadoop2.7/conf/spark-env.sh 中配置SPARK_HOME， 后者会覆盖前者

export PATH=~/bin:~/hadoop/bin:~/spark-2.4.0-bin-hadoop2.7/bin:$PATH
设置path， 可以找到spark-submit


设置python
通过环境变量设置
export PYSPARK_PYTHON=/usr/local/bin/python2.7
export PYSPARK_DRIVER_PYTHON=/usr/local/bin/python2.7
export SPARK_YARN_USER_ENV="PYSPARK_PYTHON=/usr/local/bin/python2.7"

提交程序时设置python
    spark-submit \
        --master local \
        --conf "spark.pyspark.python=/home/appops/Python/bin/python" \
        --conf "spark.pyspark.driver.python=/home/appops/Python/bin/python" \


jar包
~/spark-2.4.0-bin-hadoop2.7/jars/
- hadoop-lzo-0.4.20.jar
- mysql-connector-java-5.1.32.jar

# spark streaming

## Discretized Stream (DStream)
```
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

sc = SparkContext("local[2]", "NetworkWordCount")
ssc = StreamingContext(sc, 1)

lines = ssc.socketTextStream("localhost", 9999)

```



# Structured Streaming

```
from pyspark.sql import SparkSession
from pyspark.sql.functions import explode
from pyspark.sql.functions import split

spark = SparkSession \
    .builder \
    .appName("StructuredNetworkWordCount") \
    .getOrCreate()
```

## 数据源

socket 读取数据
```python
lines = spark \
   .readStream \
   .format("socket") \
   .option("host", "localhost") \
   .option("port", 9993) \
   .load()
```

file 读取数据
```python
lines = spark \
    .readStream \
    .format("text") \
    .load("file:///home/<path>/hello*.txt")
```
- 启动时会处理所有匹配的文件
- 运行过程中删除文件，不会影响数据
- 运行过程中不会处理被更新的旧文件

## join


## 输出

file
```python
# format can be "orc", "json", "csv", etc.
query = spark.writeStream
    .format("parquet")     
    .option("path", "path/to/destination/dir")
    .start()
```

kafka
```python
query = spark.writeStream
    .format("kafka")
    .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
    .option("topic", "updates")
    .start()
```


# spark-shell scala
交互式环境使用自定义jar包, 或第三方jar包
```
spark-shell --jars <Comma-separated list of jars>
```
运行文件
```
:help
```

# spark-shell
- deploy-mode <client|cluster, defualt=client>  
  - client: the driver runs in the client process, the application master is only used for requesting resources from YARN
  - cluster: Spark driver runs inside an application master process which is managed by YARN on the cluster
- master


https://my.oschina.net/kavn/blog/1540548


# spark job stage
- application
  - job (每个action操作触发一个job， job会被串行的提交）
    - stage (每个job会被规划为多个stage， 一些Transformation操作会形成宽依赖，需要进行shuffle时被切割成多个stage)
      - task (每个stage到底起多少个task，由输入的partition数量决定)

spark parallelism 计算
executors * cores 决定了可以同时并发运行的task数量

executor - 每个executor只会启动一个JVM， 这个executor上的所有task都会共享， broadcast时每个executor只需要一份数据
partition -    

```
# 查看分区数
df.rdd.getNumPartitions()

# 重新分区
df.repartition(10)

```