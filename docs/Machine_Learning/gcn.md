



# GCN 图卷积神经网络
- 邻接矩阵A(adjacency matrix)表示节点间的连接关系
- 度矩阵D(degree matrix)表示每个节点链接的节点数
- 特征矩阵X

- GraphConv: 
  - Graph convolutional network (GCN)
  - https://arxiv.org/abs/1609.02907

- GATConv:
  - Graph attention network (GAT)

- R
  - Relational graph convolutional network


# GraphSAGE


开源
- alibaba/euler
- alibaba/graph-learn 
  - https://github.com/alibaba/graph-learn
  - AliGraph: a comprehensive graph neural network platform
- danielegrattarola/spektral
  - https://github.com/danielegrattarola/spektral


- input：euler_ops

- model
input: (batch_size, 1)  输入节点id
gnn: (batch_size, )


graphSage 实现


数据生成过程

1. 采样batch_size个节点 (batch_size, 1)
2. batch_size个节点的一阶邻居 (batch_size, hop1_size)
3. batch_size个节点的二阶邻居 (batch_size, hop2_size)


节点特征 （batch_size, feature_dim)
一阶特征 （batch_size, hop1_size， feature_dim）
二阶特征  (batch_size, hop2_size， feature_dim)



模型结构: 由多个GCN层构成， 每个顶点都会

GCN （input_dim, 

每个节点都会使用


模型计算过程

for layer in gcn_layer:
  节点特征

  节点特征 （batch_size, feature_dim)  
  matmul(节点特征, weight_gcn_layer) + bias  =>  (batch_size, layer_1_outputdim)

  aggregator(一阶特征 （batch_size, hop1_size， feature_dim）) => 一阶邻居特征聚合 （batch_size, feature_dim）)   # mean, sum, max
  matmul(一阶邻居特征聚合, weight_aggregator) + bias => (batch_size, layer_1_outputdim)


  节点特征 和 一阶邻居特征聚合 # sum => (batch_size, layer_1_outputdim),  concat => (batch_size, layer_1_outputdim * 2)



# Graph
- multigraph
- 异构图 vs 同构图


# GraphX 
vertex: unique 64-bit long identifier

# GraphFrames

修改build.sbt, 使用匹配spark版本的scala版本

编译graphframes jar包， 跳过测试
```
./build/sbt 'set test in assembly := {}' clean assembly
```

```
pip3 install graphframes
pip3 show graphframes
使用zip打包graphframes
```

pyspark 提交任务时， 打包graphframes python包和jar包


# DGL

https://github.com/dglai/WWW20-Hands-on-Tutorial


## dgl graph

```
u, v, urange, vrange = utils.graphdata2tensors(data, idtype)

# graphdata2tensors函数的第一个返回参数为SparseAdjTuple类型
# SparseAdjTuple = namedtuple('SparseAdjTuple', ['format', 'arrays'])

```



## dgl function

`message function`:  输入(u, v, e), 输出
`reduce function`: 输入node, 聚合mailbox数据
`update function`: 


`edge-wise computation`: 调用`apply_edges`函数，参数为`message function`
`node-wise computation`：调用`update_all`函数，参数为`message function`，`reduce function`，`update function`， `update function`可以拆分开进行调用

```
import dgl.function as fn
graph.apply_edges(fn.u_add_v('el', 'er', 'e'))
```


> dgl runtime

```
When users call a method such as g.update_all(), instead of diving right to computation, DGL starts a mini-program and fills it with instructions. The instructions include read/writes on the graph features, and computation on the features with built-in operators and/or User Defined Functions. DGL then runs the program by interpreting those instructions.
```




> 模型定义  
```
class SAGENet(nn.Module):
    def __init__(self, n_layers, in_feats, out_feats, hidden_feats=None):
```
- n_layers: dglnn.SAGEConv层的数量
- in_feats：
- out_feats： 

> 模型建立
```
graph = dgl.as_heterograph(graph)

graph.ndata['features'] = graph.ndata['feat']
graph.ndata['labels'] = labels
in_feats = graph.ndata['features'].shape[1]
num_labels = len(torch.unique(labels))

HIDDEN_FEATURES = 50
model = SAGENet(2, in_feats, num_labels, HIDDEN_FEATURES)
```

> 模型训练

获取batch_size大小的block数据， 包括block和input_feature

```
output_predictions = model(blocks, input_features)
```

模型前向计算过程
```
    def forward(self, blocks, input_features):
        """
        blocks : List of blocks generated by block sampler.
        input_features : Input features of the first block.
        """
        h = input_features
        for layer, block in zip(self.convs, blocks):
            h = self.propagate(block, h, layer)
        return h

    def propagate(self, block, src_feats, layer):
        # Because GraphSAGE requires not only the features of the neighbors, but also the features
        # of the output nodes themselves on the current layer, we need to copy the output node features
        # from the input side to the output side ourselves to make GraphSAGE work correctly.
        # The output nodes of a block are guaranteed to appear the first in the input nodes, so we can
        # conveniently write like this:
        dst_feats = src_feats[:block.number_of_dst_nodes()]
        return layer(block, (src_feats, dst_feats))
```
blocks为列表， 列表长度和模型网络层数n_layer相等， 每个元素为block对应相应层的输入

block

$$
\begin{aligned}
h_{\mathcal{N}(i)}^{(l+1)} 
&= \mathrm{aggregate} \left(\{h_{j}^{l}, \forall j \in \mathcal{N}(i) \}\right)\\h_{i}^{(l+1)} 
&= \sigma \left(W \cdot \mathrm{concat}
(h_{i}^{l}, h_{\mathcal{N}(i)}^{l+1}) \right)\\h_{i}^{(l+1)} 
&= \mathrm{norm}(h_{i}^{l})
\end{aligned}
$$


> dlpack的作用

在不同框架的内存间共享tensor

> gspmm 和 gsddmm

- gspmm: Generalized Sparse Matrix Multiplication
- gsddmm: Generalized Sampled-Dense-Dense Matrix Multiplication
- gsdmm: Gibbs Sampling Dirichlet Multinomial Mixture